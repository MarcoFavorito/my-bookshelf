abstract: Using deep neural nets as function approximator for reinforcement learning
  tasks have recently been shown to be very powerful for solving problems approaching
  real-world complexity. Using these results as a benchmark, we discuss the role that
  the discount factor may play in the quality of the learning process of a deep Q-network
  (DQN). When the discount factor progressively increases up to its final value, we
  empirically show that it is possible to significantly reduce the number of learning
  steps. When used in conjunction with a varying learning rate, we empirically show
  that it outperforms original DQN on several experiments. We relate this phenomenon
  with the instabilities of neural networks when they are used in an approximate Dynamic
  Programming setting. We also describe the possibility to fall within a local optimum
  during the learning process, thus connecting our discussion with the exploration/exploitation
  dilemma.
archiveprefix: arXiv
author: François-Lavet, Vincent and Fonteneau, Raphael and Ernst, Damien
author_list:
- family: François-Lavet
  given: Vincent
- family: Fonteneau
  given: Raphael
- family: Ernst
  given: Damien
eprint: 1512.02011v2
file: 1512.02011v2.pdf
files:
- francois-lavet-vincent-and-fonteneau-raphael-and-ernst-damienhow-to-discount-deep-reinforcement-learning-towards-new-dynamic-strategies2015.pdf
month: Dec
primaryclass: cs.LG
ref: 1512.02011v2
time-added: 2020-12-07-23:46:04
title: 'How to Discount Deep Reinforcement Learning: Towards New Dynamic   Strategies'
type: article
url: http://arxiv.org/abs/1512.02011v2
year: '2015'
