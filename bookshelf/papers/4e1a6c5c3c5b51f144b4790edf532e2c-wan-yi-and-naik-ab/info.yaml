abstract: We introduce improved learning and planning algorithms for average-reward
  MDPs, including 1) the first general proven-convergent off-policy model-free control
  algorithm without reference states, 2) the first proven-convergent off-policy model-free
  prediction algorithm, and 3) the first learning algorithms that converge to the
  actual value function rather than to the value function plus an offset. All of our
  algorithms are based on using the temporal-difference error rather than the conventional
  error when updating the estimate of the average reward. Our proof techniques are
  based on those of Abounadi, Bertsekas, and Borkar (2001). Empirically, we show that
  the use of the temporal-difference error generally results in faster learning, and
  that reliance on a reference state generally results in slower learning and risks
  divergence. All of our learning algorithms are fully online, and all of our planning
  algorithms are fully incremental.
archiveprefix: arXiv
author: Wan, Yi and Naik, Abhishek and Sutton, Richard S.
author_list:
- family: Wan
  given: Yi
- family: Naik
  given: Abhishek
- family: Sutton
  given: Richard S.
eprint: 2006.16318v1
file: 2006.16318v1.pdf
files:
- wan-yi-and-naik-abhishek-and-sutton-richard-s.learning-and-planning-in-average-reward-markov-decision-processes2020.pdf
month: Jun
primaryclass: cs.LG
ref: 2006.16318v1
time-added: 2020-11-18-18:53:20
title: Learning and Planning in Average-Reward Markov Decision Processes
type: article
url: http://arxiv.org/abs/2006.16318v1
year: '2020'
