abstract: Statistical performance bounds for reinforcement learning (RL) algorithms
  can be critical for high-stakes applications like healthcare. This paper introduces
  a new framework for theoretically measuring the performance of such algorithms called
  Uniform-PAC, which is a strengthening of the classical Probably Approximately Correct
  (PAC) framework. In contrast to the PAC framework, the uniform version may be used
  to derive high probability regret guarantees and so forms a bridge between the two
  setups that has been missing in the literature. We demonstrate the benefits of the
  new framework for finite-state episodic MDPs with a new algorithm that is Uniform-PAC
  and simultaneously achieves optimal regret and PAC guarantees except for a factor
  of the horizon.
archiveprefix: arXiv
author: Dann, Christoph and Lattimore, Tor and Brunskill, Emma
author_list:
- family: Dann
  given: Christoph
- family: Lattimore
  given: Tor
- family: Brunskill
  given: Emma
eprint: 1703.07710v3
file: 1703.07710v3.pdf
files:
- dann-christoph-and-lattimore-tor-and-brunskill-emmaunifying-pac-and-regret-uniform-pac-bounds-for-episodic-reinforcement-learning2017.pdf
month: Mar
primaryclass: cs.LG
ref: 1703.07710v3
time-added: 2020-11-11-16:42:25
title: 'Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement   Learning'
type: article
url: http://arxiv.org/abs/1703.07710v3
year: '2017'
