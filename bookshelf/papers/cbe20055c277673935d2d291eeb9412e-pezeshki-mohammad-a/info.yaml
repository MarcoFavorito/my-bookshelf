abstract: We identify and formalize a fundamental gradient descent phenomenon resulting
  in a learning proclivity in over-parameterized neural networks. Gradient Starvation
  arises when cross-entropy loss is minimized by capturing only a subset of features
  relevant for the task, despite the presence of other predictive features that fail
  to be discovered. This work provides a theoretical explanation for the emergence
  of such feature imbalance in neural networks. Using tools from Dynamical Systems
  theory, we identify simple properties of learning dynamics during gradient descent
  that lead to this imbalance, and prove that such a situation can be expected given
  certain statistical structure in training data. Based on our proposed formalism,
  we develop guarantees for a novel regularization method aimed at decoupling feature
  learning dynamics, improving accuracy and robustness in cases hindered by gradient
  starvation. We illustrate our findings with simple and real-world out-of-distribution
  (OOD) generalization experiments.
archiveprefix: arXiv
author: Pezeshki, Mohammad and Kaba, Sékou-Oumar and Bengio, Yoshua and Courville,
  Aaron and Precup, Doina and Lajoie, Guillaume
author_list:
- family: Pezeshki
  given: Mohammad
- family: Kaba
  given: Sékou-Oumar
- family: Bengio
  given: Yoshua
- family: Courville
  given: Aaron
- family: Precup
  given: Doina
- family: Lajoie
  given: Guillaume
eprint: 2011.09468v1
file: 2011.09468v1.pdf
files:
- pezeshki-mohammad-and-kaba-sekou-oumar-and-bengio-yoshua-and-courville-aaron-and-precup-doina-and-lajoie-guillaumegradient-starvation-a-learnin.pdf
month: Nov
primaryclass: cs.LG
ref: 2011.09468v1
time-added: 2020-11-22-15:06:04
title: 'Gradient Starvation: A Learning Proclivity in Neural Networks'
type: article
url: http://arxiv.org/abs/2011.09468v1
year: '2020'
